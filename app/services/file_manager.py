# app/services/file_manager.py
import os
import logging
import time
import shutil
import json
import asyncio
import hashlib
from uuid import uuid4
from pathlib import Path
from typing import Optional, Dict, Any

from redis.asyncio import Redis, ConnectionError as RedisConnectionError

from app.core.config import settings
from app.core.redis_client import get_redis_client
from app.schemas.redis import AITaskPayload, UploadStatus

logger = logging.getLogger(__name__)

# Redis ìƒíƒœ ë³´ê³  í‚¤
def get_status_key(job_id: str) -> str:
    """Redisì—ì„œ ì‘ì—… ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” í‚¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return f"job:{job_id}:status"

def get_meta_key(job_id: str) -> str:
    """ì‘ì—… ë©”íƒ€ë°ì´í„°(ë©¤ë²„, í•˜ì´ë¼ì´íŠ¸í‚¤, ì›ë³¸ê²½ë¡œ)ë¥¼ ì €ì¥í•˜ëŠ” í‚¤."""
    return f"job:{job_id}:meta"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def calculate_file_checksum(file_path: Path) -> str:
    """ì£¼ì–´ì§„ íŒŒì¼ ê²½ë¡œì˜ SHA256 ì²´í¬ì„¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    hasher = hashlib.sha256()
    try:
        with file_path.open('rb') as f:
            while True:
                chunk = f.read(8192)  # 8KB ì²­í¬
                if not chunk:
                    break
                hasher.update(chunk)
        return f"sha256:{hasher.hexdigest()}"
    except Exception as e:
        logger.error(f"Failed to calculate checksum for {file_path}: {e}")
        return "sha256:error"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì§„í–‰ë¥ /ì™„ë£Œ ë³´ê³  ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def report_progress_to_spring(job_id: str, status: str, progress: float):
    """
    Redisë¥¼ í†µí•´ Spring ì„œë²„ì— ì‘ì—… ì§„í–‰ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. (0%~99% ì¤‘ê°„ ë³´ê³ ìš©)
    - key: job:{jobId}:status (ìŠ¤ëƒ…ìƒ·)
    - channel: {REDIS_UPLOAD_PROGRESS_CHANNEL}:{jobId} (Pub/Sub)
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError as e:
        logger.error(f"Redis not available, cannot report status for Job {job_id}: {e}")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    # ê³µí†µ í˜ì´ë¡œë“œ (SET / PUBLISH ë‘˜ ë‹¤ ì´ê±¸ ì‚¬ìš©)
    status_data = {
        "jobId": job_id,
        "status": status,
        "progress": f"{progress:.2f}",
        "timestamp": int(time.time()),
    }

    # 1) ìŠ¤ëƒ…ìƒ· ì €ì¥ (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    try:
        await redis.set(get_status_key(job_id), json.dumps(status_data), ex=3600)
        logger.info(f"Job {job_id} status updated: {status} ({progress:.2f}%)")
    except RedisConnectionError as e:
        logger.error(f"Redis connection dropped or operation failed for Job {job_id}: {e}")
    except Exception as e:
        logger.error(f"Failed to report status to Redis for Job {job_id}: {e}")

    # 2) Pub/Sub ì±„ë„ë¡œë„ ë°œí–‰ (ë°±ì—”ë“œê°€ SUBSCRIBE í•˜ëŠ” ìš©ë„)
    try:
        channel = f"{settings.REDIS_UPLOAD_PROGRESS_CHANNEL}:{job_id}"
        await redis.publish(channel, json.dumps(status_data))
        logger.info(
            f"Job {job_id} progress PUBLISHED to {channel}: "
            f"{status} ({progress:.2f}%)"
        )
    except Exception as e:
        logger.error(f"Failed to publish progress for Job {job_id} to channel: {e}")

async def report_final_completion_to_spring(
    job_id: str,
    final_file_path: Path,
    checksum: str,
    member_id_override: Optional[str] = None,
):
    """
    ìµœì¢… ì™„ë£Œëœ ì›ë³¸ ì˜ìƒ ì •ë³´ë¥¼ Spring ì„œë²„ì— ì•½ì†ëœ JSON í˜•ì‹ìœ¼ë¡œ Redisë¥¼ í†µí•´ ë³´ê³ í•©ë‹ˆë‹¤.
    - key: job:{jobId}:status (ìŠ¤ëƒ…ìƒ·)
    - channel: {REDIS_UPLOAD_PROGRESS_CHANNEL}:{jobId} (Pub/Sub)
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError as e:
        logger.error(f"Redis not available, cannot report final status for Job {job_id}: {e}")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    try:
        file_size_bytes = final_file_path.stat().st_size

        # SAVE_ROOT ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¥¼ ì™¸ë¶€ URLì— ë¶™ì—¬ì„œ ë…¸ì¶œ
        try:
            rel = final_file_path.relative_to(Path(settings.SAVE_ROOT))
            rel_str = str(rel).replace("\\", "/")
            source_url = f"{settings.EXTERNAL_BASE_URL}/{rel_str}"
        except Exception:
            # ìƒëŒ€ ê²½ë¡œ ê³„ì‚° ì‹¤íŒ¨ ì‹œ íŒŒì¼ëª…ë§Œ ë…¸ì¶œ(í´ë°±)
            source_url = f"{settings.EXTERNAL_BASE_URL}/{final_file_path.name}"

        member_id = member_id_override or settings.MEMBER_ID
        duration_sec = 0.0  # TODO: ì‹¤ì œ ë¶„ì„ ê°’ìœ¼ë¡œ ëŒ€ì²´(ffprobe ë“±)

    except Exception as e:
        logger.error(f"Failed to get file stats for final notification: {e}")
        file_size_bytes = 0
        source_url = "error_url"
        member_id = member_id_override or settings.MEMBER_ID
        duration_sec = 0.0

    payload = {
        "status": 200,
        "success": True,
        "data": {
            "type": "UPLOAD_COMPLETE",
            "memberId": member_id,
            "jobId": job_id,
            "sizeBytes": file_size_bytes,
            "sourceUrl": source_url,
            "checksum": checksum,
            "durationSec": duration_sec,
        },
        "message": "Original video successfully merged and stored.",
        "timestamp": int(time.time() * 1000),
    }

    # 1) ìŠ¤ëƒ…ìƒ· ì €ì¥ (ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    try:
        await redis.set(get_status_key(job_id), json.dumps(payload), ex=3600)
        logger.info(f"Final completion JSON reported to Redis for Job {job_id}.")
    except Exception as e:
        logger.error(f"Failed to report final completion JSON to Redis for Job {job_id}: {e}")

    # 2) Pub/Sub ì±„ë„ë¡œë„ ë°œí–‰ (ë°±ì—”ë“œì—ì„œ SUBSCRIBEë¡œ ë°›ëŠ” ìš©ë„)
    try:
        channel = f"{settings.REDIS_UPLOAD_PROGRESS_CHANNEL}:{job_id}"
        await redis.publish(channel, json.dumps(payload))
        logger.info(
            f"Final completion JSON PUBLISHED to channel {channel} for Job {job_id}."
        )
    except Exception as e:
        logger.error(
            f"Failed to publish final completion JSON to channel for Job {job_id}: {e}"
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”íƒ€ ì €ì¥ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _save_job_meta(job_id: str, meta: Dict[str, Any]) -> None:
    """AI ì›Œì»¤/ë°±ì—”ë“œê°€ ì¡°íšŒí•  ìˆ˜ ìˆë„ë¡ ì‘ì—… ë©”íƒ€ë¥¼ Redisì— ì €ì¥."""
    try:
        redis: Redis = get_redis_client()
    except Exception as e:
        logger.warning(f"Redis unavailable; meta not saved for {job_id}: {e}")
        return
    try:
        await redis.set(get_meta_key(job_id), json.dumps(meta), ex=86400)  # 1 day
        logger.info(f"Saved job meta for {job_id}: {meta}")
    except Exception as e:
        logger.warning(f"Failed to save job meta for {job_id}: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI Worker ì—°ë™ ë¡œì§ (Redis Queue)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _trigger_ai_worker(job_id: str, final_file_path: Path, member_id: Optional[str], highlight_identifier: str):
    """
    AI Workerê°€ ì²˜ë¦¬í•  ì‘ì—… ìš”ì²­ì„ Redis List(Queue)ì— í‘¸ì‹œí•˜ê³ ,
    Spring ì„œë²„ì— AI ì‘ì—… ì‹œì‘ ëŒ€ê¸° ìƒíƒœë¥¼ ë³´ê³ í•©ë‹ˆë‹¤.
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError:
        logger.error(f"Redis not available, cannot queue AI task for Job {job_id}.")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    # 1. AI Workerì—ê²Œ ì „ë‹¬í•  í˜ì´ë¡œë“œ êµ¬ì„±
    ai_payload = AITaskPayload(
        jobId=job_id,
        memberId=(member_id or settings.MEMBER_ID),
        # originalFilePath=str(final_file_path.resolve())  # â¬…ï¸ ì´ì „(ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ˆëŒ€ ê²½ë¡œ ìœ„í—˜)
        originalFilePath=str(final_file_path)  # â¬…ï¸ ê³µìœ  ë³¼ë¥¨ ê²½ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬
    )

    # extra í•„ë“œ(ì›Œì»¤ ëª¨ë¸ì— ì—†ì–´ë„ ë¬´ì‹œë˜ë„ë¡ JSONì—ë§Œ í¬í•¨)
    payload_dict = json.loads(ai_payload.model_dump_json())
    payload_dict["highlightKey"] = highlight_identifier  # ì¶”ê°€ ë©”íƒ€

    try:
        push_count = await redis.rpush(
            getattr(settings, "REDIS_QUEUE_NAME", "opencv-ai-job-queue"),
            json.dumps(payload_dict)
        )
        logger.info(
            f"AI Task for Job {job_id} pushed to queue '{getattr(settings, 'REDIS_QUEUE_NAME', 'opencv-ai-job-queue')}'. "
            f"Queue size: {push_count}"
        )

        # AI ì‹œì‘ ëŒ€ê¸° ìƒíƒœ: ì—¬ê¸°ì„œëŠ” ê·¸ëŒ€ë¡œ 100% (ì—…ë¡œë“œ ë‹¨ê³„ëŠ” ì´ë¯¸ ëë‚œ ìƒíƒœ)
        await report_progress_to_spring(job_id, UploadStatus.AI_START_PENDING.value, 100.0)

    except Exception as e:
        logger.error(f"Failed to queue AI task for Job {job_id}: {e}")
        await report_progress_to_spring(job_id, UploadStatus.ERROR.value, 0.0)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: ë³‘í•© ë° ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def merge_chunks_and_cleanup(
    job_id: str,
    file_name: str,
    total_parts: int,
    chunk_dir: Path,
    member_id: Optional[str] = None,   # â† A-2: ì„ íƒ ì¸ì ì¶”ê°€ (ê¸°ì¡´ í˜¸ì¶œê³¼ í˜¸í™˜)
):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ë³‘í•© ì‘ì—… ë¡œì§ì…ë‹ˆë‹¤.
    - ì²­í¬ë¥¼ ì„ì‹œ ê²½ë¡œì— ë³‘í•©í•œ ë’¤
    - ê³µìœ  ë³¼ë¥¨(SAVE_ROOT/{job_id}/{file_name})ìœ¼ë¡œ ì´ë™
    - ì™„ë£Œ ì •ë³´ Redis ë³´ê³  ë° AI Worker í íŠ¸ë¦¬ê±°

    ğŸ”¹ ì²­í¬ ì—…ë¡œë“œ êµ¬ê°„ì—ì„œ ì´ë¯¸ 0~90% ì§„í–‰ë¥ ì„ ë³´ê³ í•˜ê³  ìˆìœ¼ë¯€ë¡œ,
       ì´ í•¨ìˆ˜ì—ì„œëŠ”:
       - ë³‘í•© ì™„ë£Œ ì‹œ 99% (PROCESSING)
       - ìµœì¢… ì €ì¥ ì™„ë£Œ ì‹œ 100% (UPLOAD_COMPLETE)
       ë§Œ ì¶”ê°€ë¡œ ë³´ê³ í•©ë‹ˆë‹¤.
    """
    # ìµœì¢… ì €ì¥ ë””ë ‰í† ë¦¬(SAVE_ROOT/{job_id})
    final_save_dir = Path(settings.SAVE_ROOT) / job_id
    final_save_dir.mkdir(parents=True, exist_ok=True)

    # ìµœì¢… ì €ì¥ ê²½ë¡œ(SAVE_ROOT/{job_id}/{file_name})
    final_save_path = final_save_dir / file_name

    # ì„ì‹œ ë³‘í•© ê²½ë¡œ(TEMP_ROOT/temp_{job_id}_{file_name})
    temp_merge_path = Path(settings.TEMP_ROOT) / f"temp_{job_id}_{file_name}"

    final_path: Optional[Path] = None
    calculated_checksum: Optional[str] = None

    # âš ï¸ ì—¬ê¸°ì„œëŠ” ë” ì´ìƒ 0% ì´ˆê¸°í™” í˜¸ì¶œì„ í•˜ì§€ ì•ŠìŒ
    #    (ì²­í¬ ì—…ë¡œë“œ ë‹¨ê³„ì—ì„œ ì´ë¯¸ 0~90%ë¥¼ ë³´ê³ í•˜ê³  ìˆìŒ)

    try:
        # 2) ì²­í¬ íŒŒì¼ ëª©ë¡ ì •ë ¬ ë° ê²€ì¦
        #   - ì—…ë¡œë“œ ë¼ìš°í„°ì—ì„œ '{job_id}_{file_name}.{part_index}' í˜•íƒœë¡œ ì €ì¥í–ˆë‹¤ê³  ê°€ì •
        chunk_files = sorted(chunk_dir.glob(f"{job_id}_{file_name}.*"))
        if len(chunk_files) != total_parts:
            raise Exception(
                f"Integrity check failure during merge. expected={total_parts}, actual={len(chunk_files)}"
            )

        logger.info(f"Starting merge of {total_parts} chunks into {temp_merge_path}")

        # 3) ì„ì‹œ ê²½ë¡œì— ë³‘í•©
        #    ë³‘í•© ì¤‘ ì„¸ë¶€ ì§„í–‰ë¥ ì€ ë³„ë„ë¡œ ë³´ë‚´ì§€ ì•Šê³ ,
        #    ì „ì²´ ë³‘í•©ì´ ëë‚œ ì‹œì ì— 99%ë¡œ ì¼ê´„ ë³´ê³ 
        with temp_merge_path.open("wb") as outfile:
            for chunk_file in chunk_files:
                with chunk_file.open("rb") as infile:
                    shutil.copyfileobj(infile, outfile)

        logger.info("Merge completed at temp location. Moving to final save dir and calculating checksum.")

        # 3.2) ë³‘í•© ì™„ë£Œ ì‹œì : 99% ë³´ê³  (PROCESSING ìƒíƒœ)
        try:
            await report_progress_to_spring(
                job_id,
                UploadStatus.PROCESSING.value,  # "PROCESSING"
                99.0,
            )
        except Exception as e:
            logger.error(f"Failed to report 99% PROCESSING for Job {job_id}: {e}")

        # 3.5) ìµœì¢… ê²½ë¡œë¡œ ì´ë™ (SAVE_ROOT/{jobId}/{file_name})
        shutil.move(str(temp_merge_path), str(final_save_path))
        final_path = final_save_path

        # 4) ì›ë³¸ì´ ìµœì¢… SAVE_ROOT ê²½ë¡œì— ì €ì¥ëœ ì‹œì : 100% ë³´ê³  (UPLOAD_COMPLETE ìƒíƒœ)
        try:
            await report_progress_to_spring(
                job_id,
                UploadStatus.UPLOAD_COMPLETE.value,  # "UPLOAD_COMPLETE"
                100.0,
            )
        except Exception as e:
            logger.error(f"Failed to report 100% UPLOAD_COMPLETE for Job {job_id}: {e}")

        # 4.5) ì²´í¬ì„¬ ê³„ì‚° (ì§„í–‰ë¥ ì—ëŠ” ì˜í–¥ ì—†ìŒ)
        calculated_checksum = calculate_file_checksum(final_path)

        # 4.6) í•˜ì´ë¼ì´íŠ¸ ì‹ë³„ì ìƒì„± + ë©”íƒ€ ì €ì¥
        highlight_identifier = str(uuid4())
        await _save_job_meta(
            job_id,
            {
                "memberId": (member_id or settings.MEMBER_ID),
                "highlightIdentifier": highlight_identifier,
                "originalFilePath": str(final_path),
            },
        )

        # 5) ìµœì¢… ì™„ë£Œ JSON ë³´ê³  (ë©¤ë²„ ë°˜ì˜)
        await report_final_completion_to_spring(
            job_id,
            final_path,
            calculated_checksum,
            member_id_override=member_id,
        )

        # 6) AI Worker í í‘¸ì‹œ ë° ìƒíƒœ ë³´ê³  (í•˜ì´ë¼ì´íŠ¸í‚¤ í¬í•¨)
        await _trigger_ai_worker(job_id, final_path, member_id, highlight_identifier)

    except Exception as e:
        logger.error(
            f"Critical error during merge/cleanup/trigger for Job {job_id}: {e}",
            exc_info=True
        )
        # ì—ëŸ¬ ì‹œ ê¸°ì¡´ ë¬¸ìì—´ ìƒíƒœ ìœ ì§€ ("FAILED") â€” ë°±ì—”ë“œê°€ ì´ ê°’ì— ë§ì¶°ìˆì„ ìˆ˜ ìˆì–´ì„œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
        await report_progress_to_spring(job_id, "FAILED", 0.0)

    finally:
        # 7) ì„ì‹œ ì²­í¬ í´ë” ì‚­ì œ
        try:
            if chunk_dir.exists():
                shutil.rmtree(chunk_dir, ignore_errors=True)
                logger.info(f"Cleanup complete for Job {job_id}: removed chunk dir {chunk_dir}")
        except Exception as e:
            logger.warning(f"Failed to remove chunk dir {chunk_dir} for Job {job_id}: {e}")

        # 8) ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” ì„ì‹œ ë³‘í•© íŒŒì¼ ì œê±°(ì´ë™ ì‹¤íŒ¨ ë“±ì˜ ê²½ìš°)
        try:
            if temp_merge_path.exists():
                os.remove(temp_merge_path)
                logger.info(f"Cleanup complete for Job {job_id}: removed temp file {temp_merge_path}")
        except Exception as e:
            logger.warning(f"Failed to remove temp file {temp_merge_path} for Job {job_id}: {e}")
