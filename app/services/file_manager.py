# app/services/file_manager.py
import os
import logging
import time
import shutil
import json
import asyncio
import hashlib
from uuid import uuid4
from pathlib import Path
from typing import Optional, Dict, Any

from redis.asyncio import Redis, ConnectionError as RedisConnectionError

from app.core.config import settings
from app.core.redis_client import get_redis_client
from app.schemas.redis import AITaskPayload, UploadStatus

logger = logging.getLogger(__name__)

# Redis ìƒíƒœ ë³´ê³  í‚¤
def get_status_key(job_id: str) -> str:
    """Redisì—ì„œ ì‘ì—… ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” í‚¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return f"job:{job_id}:status"


def get_meta_key(job_id: str) -> str:
    """ì‘ì—… ë©”íƒ€ë°ì´í„°(ë©¤ë²„, í•˜ì´ë¼ì´íŠ¸í‚¤, ì›ë³¸ê²½ë¡œ)ë¥¼ ì €ì¥í•˜ëŠ” í‚¤."""
    return f"job:{job_id}:meta"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def calculate_file_checksum(file_path: Path) -> str:
    """ì£¼ì–´ì§„ íŒŒì¼ ê²½ë¡œì˜ SHA256 ì²´í¬ì„¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    hasher = hashlib.sha256()
    try:
        with file_path.open("rb") as f:
            while True:
                chunk = f.read(8192)  # 8KB ì²­í¬
                if not chunk:
                    break
                hasher.update(chunk)
        return f"sha256:{hasher.hexdigest()}"
    except Exception as e:
        logger.error(f"Failed to calculate checksum for {file_path}: {e}")
        return "sha256:error"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì§„í–‰ë¥ /ì™„ë£Œ ë³´ê³  ë¡œì§ (Spring ProgressData ê·œê²©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def report_progress_to_spring(
    job_id: str,
    progress_type: str,
    progress: Optional[float] = None,
    *,
    member_id: Optional[str] = None,
    total_bytes: Optional[int] = None,      # â† ì¸ìëŠ” ìœ ì§€í•˜ì§€ë§Œ
    received_bytes: Optional[int] = None,   #    ì•„ë˜ payload ì—ì„œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    size_bytes: Optional[int] = None,
    checksum: Optional[str] = None,
    duration_sec: Optional[float] = None,
    stage: Optional[str] = None,
    current_clip: Optional[int] = None,
    total_clips: Optional[int] = None,
) -> None:
    """
    Spring ProgressData ìµœì¢… ê·œê²©ì— ë§ì¶˜ ê³µí†µ ë³´ê³  í•¨ìˆ˜.

    ğŸ‘‰ Redis/PubSub ë¡œ ë‚˜ê°€ëŠ” JSON í˜•ì‹ì€ íƒ€ì…ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ ì œí•œë¨.

    1) ì²­í¬ ì—…ë¡œë“œ ì¤‘ (UPLOADING)
    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990000000,
      "type": "UPLOADING",
      "progress": 32.5,
      "jobId": "job1",
      "memberId": "xxxx"
    }

    2) ì—…ë¡œë“œ/ë³‘í•© ì™„ë£Œ (UPLOAD_COMPLETE)
    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990001000,
      "type": "UPLOAD_COMPLETE",
      "jobId": "job1",
      "memberId": "xxxx"
    }

    3) AI ì²˜ë¦¬ ì¤‘ (PROCESSING)
    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990002000,
      "type": "PROCESSING",
      "progress": 32.5,
      "jobId": "job1",
      "memberId": "xxxx"
    }

    4) AI ì²˜ë¦¬ ì™„ë£Œ (COMPLETE)
    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990003000,
      "type": "COMPLETE",
      "jobId": "job1",
      "memberId": "xxxx"
    }

    ğŸ”¹ ê·¸ ì™¸ì˜ í•„ë“œ(totalBytes, sizeBytes, checksum, stage, currentClip, totalClips ë“±)ëŠ”
       ë°±ì—”ë“œ ìŠ¤í™ì— ë§ì¶”ê¸° ìœ„í•´ ì „ì†¡í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError as e:
        logger.error(f"Redis not available, cannot report status for Job {job_id}: {e}")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    # íƒ€ì… ë¬¸ìì—´ ì •ê·œí™”
    normalized_type = str(progress_type)

    # ê³µí†µ í•„ë“œ (í•­ìƒ 6ê°œ ê³ ì •)
    payload: Dict[str, Any] = {
        "status": 200,
        "success": True,
        "timeStamp": int(time.time() * 1000),          # ms ë‹¨ìœ„
        "type": normalized_type,
        "jobId": str(job_id),
        "memberId": str(member_id or settings.MEMBER_ID),
    }

    # ğŸ”¹ progress ëŠ” UPLOADING / PROCESSING ì¼ ë•Œë§Œ í¬í•¨
    if normalized_type in ("UPLOADING", "PROCESSING") and progress is not None:
        payload["progress"] = float(progress)

    # (ë‚˜ë¨¸ì§€ totalBytes, sizeBytes, checksum, stage ë“±ì€
    #  ì¸ìë¡œë§Œ ë°›ê³  payload ì—ëŠ” ë„£ì§€ ì•ŠëŠ”ë‹¤.)

    # 1) ìŠ¤ëƒ…ìƒ· ì €ì¥
    try:
        await redis.set(get_status_key(job_id), json.dumps(payload), ex=3600)
        logger.info(
            f"Job {job_id} status updated: type={normalized_type}, "
            f"progress={payload.get('progress')}"
        )
    except RedisConnectionError as e:
        logger.error(f"Redis connection dropped or operation failed for Job {job_id}: {e}")
    except Exception as e:
        logger.error(f"Failed to report status to Redis for Job {job_id}: {e}")

    # 2) Pub/Sub ë°œí–‰
    try:
        channel = f"{settings.REDIS_UPLOAD_PROGRESS_CHANNEL}:{job_id}"
        await redis.publish(channel, json.dumps(payload))
        logger.info(
            f"Job {job_id} progress PUBLISHED to {channel}: "
            f"type={normalized_type}, progress={payload.get('progress')}"
        )
    except Exception as e:
        logger.error(f"Failed to publish progress for Job {job_id} to channel: {e}")


async def report_final_completion_to_spring(
    job_id: str,
    final_file_path: Path,
    checksum: str,
    member_id_override: Optional[str] = None,
) -> None:
    """
    UPLOAD_COMPLETE ë‹¨ê³„(ì›ë³¸ ì˜ìƒ ë³‘í•© + SAVE_ROOTì— ìµœì¢… ì €ì¥ ì™„ë£Œ)ì— ëŒ€í•œ
    ìµœì¢… ì™„ë£Œ ë³´ê³ .

    ğŸ‘‰ ìµœì¢…ì ìœ¼ë¡œëŠ” type="UPLOAD_COMPLETE", progress ì—†ì´
       {status, success, timeStamp, type, jobId, memberId} ë§Œ ì „ì†¡ëœë‹¤.
    """
    try:
        # ì§€ê¸ˆì€ ì‚¬ì´ì¦ˆ/ê¸¸ì´/ì²´í¬ì„¬ì„ Redis payload ë¡œ ë³´ë‚´ì§€ ì•ŠëŠ”ë‹¤ (ìŠ¤í™ ìµœì†Œí™”)
        _ = final_file_path.stat().st_size  # í•„ìš”í•˜ë©´ ë¡œì»¬ ë¡œê·¸ ì •ë„ë¡œë§Œ í™œìš© ê°€ëŠ¥
        member_id = member_id_override or settings.MEMBER_ID
    except Exception as e:
        logger.error(f"Failed to get file stats for final notification: {e}")
        member_id = member_id_override or settings.MEMBER_ID

    # UploadStatus ì— UPLOAD_COMPLETE ê°€ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¬¸ìì—´ ì‚¬ìš©
    try:
        progress_type = UploadStatus.UPLOAD_COMPLETE.value  # type: ignore[attr-defined]
    except Exception:
        progress_type = "UPLOAD_COMPLETE"

    try:
        # ğŸ”¹ UPLOAD_COMPLETE ì—ì„œëŠ” progress ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ
        await report_progress_to_spring(
            job_id,
            progress_type,
            None,
            member_id=member_id,
        )
        logger.info(f"Final completion reported to Redis for Job {job_id}.")
    except Exception as e:
        logger.error(f"Failed to report final completion to Redis for Job {job_id}: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”íƒ€ ì €ì¥ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _save_job_meta(job_id: str, meta: Dict[str, Any]) -> None:
    """AI ì›Œì»¤/ë°±ì—”ë“œê°€ ì¡°íšŒí•  ìˆ˜ ìˆë„ë¡ ì‘ì—… ë©”íƒ€ë¥¼ Redisì— ì €ì¥."""
    try:
        redis: Redis = get_redis_client()
    except Exception as e:
        logger.warning(f"Redis unavailable; meta not saved for {job_id}: {e}")
        return
    try:
        await redis.set(get_meta_key(job_id), json.dumps(meta), ex=86400)  # 1 day
        logger.info(f"Saved job meta for {job_id}: {meta}")
    except Exception as e:
        logger.warning(f"Failed to save job meta for {job_id}: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI Worker ì—°ë™ ë¡œì§ (Redis Queue)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _trigger_ai_worker(
    job_id: str,
    final_file_path: Path,
    member_id: Optional[str],
    highlight_identifier: str,
) -> None:
    """
    AI Workerê°€ ì²˜ë¦¬í•  ì‘ì—… ìš”ì²­ì„ Redis List(Queue)ì— í‘¸ì‹œí•˜ê³ ,
    Spring ì„œë²„ì— 'í•˜ì´ë¼ì´íŠ¸ ìƒì„± ëŒ€ê¸°(QUEUEì— ìŒ“ì¸ ìƒíƒœ)' ì§„í–‰ ìƒíƒœë¥¼ ë³´ê³ .
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError:
        logger.error(f"Redis not available, cannot queue AI task for Job {job_id}.")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    # 1. AI Workerì—ê²Œ ì „ë‹¬í•  í˜ì´ë¡œë“œ êµ¬ì„±
    ai_payload = AITaskPayload(
        jobId=job_id,
        memberId=(member_id or settings.MEMBER_ID),
        # originalFilePath=str(final_file_path.resolve())  # â¬…ï¸ ì´ì „(ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ˆëŒ€ ê²½ë¡œ ìœ„í—˜)
        originalFilePath=str(final_file_path),  # â¬…ï¸ ê³µìœ  ë³¼ë¥¨ ê²½ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬
    )

    # extra í•„ë“œ(ì›Œì»¤ ëª¨ë¸ì— ì—†ì–´ë„ ë¬´ì‹œë˜ë„ë¡ JSONì—ë§Œ í¬í•¨)
    payload_dict = json.loads(ai_payload.model_dump_json())
    payload_dict["highlightKey"] = highlight_identifier  # ì¶”ê°€ ë©”íƒ€

    try:
        queue_name = getattr(settings, "REDIS_QUEUE_NAME", "opencv-ai-job-queue")
        push_count = await redis.rpush(queue_name, json.dumps(payload_dict))
        logger.info(
            f"AI Task for Job {job_id} pushed to queue '{queue_name}'. "
            f"Queue size: {push_count}"
        )

        # ğŸ”¹ í•˜ì´ë¼ì´íŠ¸ ìƒì„± ë‹¨ê³„ ì‹œì‘ ì „, type="PROCESSING", progress=0 ìœ¼ë¡œ í•œ ë²ˆ ë³´ê³ 
        try:
            try:
                processing_type = UploadStatus.PROCESSING.value  # type: ignore[attr-defined]
            except Exception:
                processing_type = "PROCESSING"

            await report_progress_to_spring(
                job_id,
                processing_type,
                0.0,
                member_id=(member_id or settings.MEMBER_ID),
            )
        except Exception as e:
            logger.error(f"Failed to report PROCESSING(QUEUED) for Job {job_id}: {e}")

    except Exception as e:
        logger.error(f"Failed to queue AI task for Job {job_id}: {e}")
        # ì—ëŸ¬ ì‹œì—ëŠ” ê¸°ì¡´ ERROR/FAILED í”Œë¡œìš°ì™€ í˜¸í™˜ë˜ë„ë¡ ë¬¸ìì—´ ì‚¬ìš©
        try:
            error_type = UploadStatus.ERROR.value  # type: ignore[attr-defined]
        except Exception:
            error_type = "FAILED"
        await report_progress_to_spring(job_id, error_type, None)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: ë³‘í•© ë° ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def merge_chunks_and_cleanup(
    job_id: str,
    file_name: str,
    total_parts: int,
    chunk_dir: Path,
    member_id: Optional[str] = None,  # â† ê¸°ì¡´ í˜¸ì¶œê³¼ í˜¸í™˜
) -> None:
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ë³‘í•© ì‘ì—… ë¡œì§ì…ë‹ˆë‹¤.
    - ì²­í¬ë¥¼ ì„ì‹œ ê²½ë¡œì— ë³‘í•©í•œ ë’¤
    - ê³µìœ  ë³¼ë¥¨(SAVE_ROOT/{job_id}/{file_name})ìœ¼ë¡œ ì´ë™
    - ì™„ë£Œ ì •ë³´ Redis ë³´ê³  ë° AI Worker í íŠ¸ë¦¬ê±°

    ğŸ”¹ ì²­í¬ ì—…ë¡œë“œ êµ¬ê°„ì—ì„œ ì´ë¯¸ 0~90% ì§„í–‰ë¥ ì„ ë³´ê³ í•˜ê³  ìˆìœ¼ë¯€ë¡œ,
       ì´ í•¨ìˆ˜ì—ì„œëŠ”:
       - ë³‘í•© ì™„ë£Œ ì‹œ PROCESSING 99% (ì—¬ê¸°ì„œë„ type=PROCESSING, progress í¬í•¨)
       - ìµœì¢… ì €ì¥ ì™„ë£Œ ì‹œ UPLOAD_COMPLETE (progress ì—†ì´)
       ë§Œ ì¶”ê°€ë¡œ ë³´ê³ .
    """
    # ìµœì¢… ì €ì¥ ë””ë ‰í† ë¦¬(SAVE_ROOT/{job_id})
    final_save_dir = Path(settings.SAVE_ROOT) / job_id
    final_save_dir.mkdir(parents=True, exist_ok=True)

    # ìµœì¢… ì €ì¥ ê²½ë¡œ(SAVE_ROOT/{job_id}/{file_name})
    final_save_path = final_save_dir / file_name

    # ì„ì‹œ ë³‘í•© ê²½ë¡œ(TEMP_ROOT/temp_{job_id}_{file_name})
    temp_merge_path = Path(settings.TEMP_ROOT) / f"temp_{job_id}_{file_name}"

    final_path: Optional[Path] = None
    calculated_checksum: Optional[str] = None

    try:
        # 2) ì²­í¬ íŒŒì¼ ëª©ë¡ ì •ë ¬ ë° ê²€ì¦
        chunk_files = sorted(chunk_dir.glob(f"{job_id}_{file_name}.*"))
        if len(chunk_files) != total_parts:
            raise Exception(
                f"Integrity check failure during merge. expected={total_parts}, actual={len(chunk_files)}"
            )

        logger.info(f"Starting merge of {total_parts} chunks into {temp_merge_path}")

        # 3) ì„ì‹œ ê²½ë¡œì— ë³‘í•©
        with temp_merge_path.open("wb") as outfile:
            for chunk_file in chunk_files:
                with chunk_file.open("rb") as infile:
                    shutil.copyfileobj(infile, outfile)

        logger.info("Merge completed at temp location. Moving to final save dir and calculating checksum.")

        # 3.2) ë³‘í•© ì™„ë£Œ ì‹œì : PROCESSING 99%
        try:
            try:
                processing_type = UploadStatus.PROCESSING.value  # type: ignore[attr-defined]
            except Exception:
                processing_type = "PROCESSING"

            await report_progress_to_spring(
                job_id,
                processing_type,
                99.0,
                member_id=(member_id or settings.MEMBER_ID),
            )
        except Exception as e:
            logger.error(f"Failed to report 99% PROCESSING for Job {job_id}: {e}")

        # 3.5) ìµœì¢… ê²½ë¡œë¡œ ì´ë™ (SAVE_ROOT/{jobId}/{file_name})
        shutil.move(str(temp_merge_path), str(final_save_path))
        final_path = final_save_path

        # 4) ì²´í¬ì„¬ ê³„ì‚° (í˜„ì¬ëŠ” Redis payload ì— ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, í•„ìš” ì‹œ ë¡œê·¸/ê²€ì¦ìš©)
        calculated_checksum = calculate_file_checksum(final_path)

        # 5) ìµœì¢… ì™„ë£Œ JSON ë³´ê³  (type=UPLOAD_COMPLETE, progress ì—†ìŒ)
        await report_final_completion_to_spring(
            job_id,
            final_path,
            calculated_checksum,
            member_id_override=member_id,
        )

        # 6) AI Worker í í‘¸ì‹œ ë° ìƒíƒœ ë³´ê³  (í•˜ì´ë¼ì´íŠ¸í‚¤ í¬í•¨)
        await _trigger_ai_worker(job_id, final_path, member_id, highlight_identifier=str(uuid4()))

    except Exception as e:
        logger.error(
            f"Critical error during merge/cleanup/trigger for Job {job_id}: {e}",
            exc_info=True,
        )
        # ì—ëŸ¬ ì‹œ ê¸°ì¡´ ë¬¸ìì—´ ìƒíƒœ ìœ ì§€ ("FAILED")
        await report_progress_to_spring(job_id, "FAILED", None)
    finally:
        # 7) ì„ì‹œ ì²­í¬ í´ë” ì‚­ì œ
        try:
            if chunk_dir.exists():
                shutil.rmtree(chunk_dir, ignore_errors=True)
                logger.info(f"Cleanup complete for Job {job_id}: removed chunk dir {chunk_dir}")
        except Exception as e:
            logger.warning(f"Failed to remove chunk dir {chunk_dir} for Job {job_id}: {e}")

        # 8) ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” ì„ì‹œ ë³‘í•© íŒŒì¼ ì œê±°(ì´ë™ ì‹¤íŒ¨ ë“±ì˜ ê²½ìš°)
        try:
            if temp_merge_path.exists():
                os.remove(temp_merge_path)
                logger.info(f"Cleanup complete for Job {job_id}: removed temp file {temp_merge_path}")
        except Exception as e:
            logger.warning(f"Failed to remove temp file {temp_merge_path} for Job {job_id}: {e}")
