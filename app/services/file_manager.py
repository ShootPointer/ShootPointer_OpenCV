# app/services/file_manager.py
import os
import logging
import time
import shutil
import json
import asyncio
import hashlib
from uuid import uuid4
from pathlib import Path
from typing import Optional, Dict, Any

from redis.asyncio import Redis, ConnectionError as RedisConnectionError

from app.core.config import settings
from app.core.redis_client import get_redis_client
from app.schemas.redis import AITaskPayload, UploadStatus

logger = logging.getLogger(__name__)

# Redis ìƒíƒœ ë³´ê³  í‚¤
def get_status_key(job_id: str) -> str:
    """Redisì—ì„œ ì‘ì—… ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” í‚¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return f"job:{job_id}:status"


def get_meta_key(job_id: str) -> str:
    """ì‘ì—… ë©”íƒ€ë°ì´í„°(ë©¤ë²„, í•˜ì´ë¼ì´íŠ¸í‚¤, ì›ë³¸ê²½ë¡œ)ë¥¼ ì €ì¥í•˜ëŠ” í‚¤."""
    return f"job:{job_id}:meta"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def calculate_file_checksum(file_path: Path) -> str:
    """ì£¼ì–´ì§„ íŒŒì¼ ê²½ë¡œì˜ SHA256 ì²´í¬ì„¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    hasher = hashlib.sha256()
    try:
        with file_path.open("rb") as f:
            while True:
                chunk = f.read(8192)  # 8KB ì²­í¬
                if not chunk:
                    break
                hasher.update(chunk)
        return f"sha256:{hasher.hexdigest()}"
    except Exception as e:
        logger.error(f"Failed to calculate checksum for {file_path}: {e}")
        return "sha256:error"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì§„í–‰ë¥ /ì™„ë£Œ ë³´ê³  ë¡œì§ (Spring ProgressData ê·œê²©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def report_progress_to_spring(
    job_id: str,
    progress_type: str,
    progress: Optional[float] = None,
    *,
    member_id: Optional[str] = None,
    total_bytes: Optional[int] = None,
    received_bytes: Optional[int] = None,
    size_bytes: Optional[int] = None,
    checksum: Optional[str] = None,
    duration_sec: Optional[float] = None,
    stage: Optional[str] = None,
    current_clip: Optional[int] = None,
    total_clips: Optional[int] = None,
) -> None:
    """
    Spring ProgressData + ProgressType ê·œê²©ì— ë§ì¶˜ ê³µí†µ ë³´ê³  í•¨ìˆ˜.

    Redisì— ì €ì¥/ë°œí–‰ë˜ëŠ” JSON í˜•ì‹(ìµœì¢… í•©ì˜ëœ í˜•íƒœ):

    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990000000,
      "type": "UPLOADING" | "UPLOAD_COMPLETE" | "PROCESSING" | "COMPLETE",
      "memberId": "...",
      "jobId": "...",

      // íƒ€ì…ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
      "progress": 32.5,
      "totalBytes": 123456,
      "receivedBytes": 32768,
      "sizeBytes": 123456,
      "checksum": "sha256:....",
      "durationSec": 0.0,
      "stage": "QUEUED" | "ANALYZING" | "CUTTING" ...,
      "currentClip": 1,
      "totalClips": 10
    }
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError as e:
        logger.error(f"Redis not available, cannot report status for Job {job_id}: {e}")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    # ê³µí†µ í•„ë“œ (Spring ProgressData + envelope)
    payload: Dict[str, Any] = {
        "status": 200,
        "success": True,
        "timeStamp": int(time.time() * 1000),  # ms ë‹¨ìœ„
        "type": progress_type,
        "memberId": member_id or settings.MEMBER_ID,
        "jobId": job_id,
    }

    # ì„ íƒ í•„ë“œë“¤(í•´ë‹¹ íƒ€ì…ì— í•„ìš”í•  ë•Œë§Œ ì±„ì›€)
    if progress is not None:
        payload["progress"] = float(progress)

    if total_bytes is not None:
        payload["totalBytes"] = int(total_bytes)

    if received_bytes is not None:
        payload["receivedBytes"] = int(received_bytes)

    if size_bytes is not None:
        payload["sizeBytes"] = int(size_bytes)

    if checksum is not None:
        payload["checksum"] = checksum

    if duration_sec is not None:
        payload["durationSec"] = float(duration_sec)

    if stage is not None:
        payload["stage"] = stage

    if current_clip is not None:
        payload["currentClip"] = int(current_clip)

    if total_clips is not None:
        payload["totalClips"] = int(total_clips)

    # 1) ìŠ¤ëƒ…ìƒ· ì €ì¥
    try:
        await redis.set(get_status_key(job_id), json.dumps(payload), ex=3600)
        logger.info(
            f"Job {job_id} status updated: type={progress_type}, "
            f"progress={payload.get('progress')}, stage={payload.get('stage')}"
        )
    except RedisConnectionError as e:
        logger.error(f"Redis connection dropped or operation failed for Job {job_id}: {e}")
    except Exception as e:
        logger.error(f"Failed to report status to Redis for Job {job_id}: {e}")

    # 2) Pub/Sub ë°œí–‰
    try:
        channel = f"{settings.REDIS_UPLOAD_PROGRESS_CHANNEL}:{job_id}"
        await redis.publish(channel, json.dumps(payload))
        logger.info(
            f"Job {job_id} progress PUBLISHED to {channel}: "
            f"type={progress_type}, progress={payload.get('progress')}, stage={payload.get('stage')}"
        )
    except Exception as e:
        logger.error(f"Failed to publish progress for Job {job_id} to channel: {e}")


async def report_final_completion_to_spring(
    job_id: str,
    final_file_path: Path,
    checksum: str,
    member_id_override: Optional[str] = None,
) -> None:
    """
    UPLOAD_COMPLETE ë‹¨ê³„(ì›ë³¸ ì˜ìƒ ë³‘í•© + SAVE_ROOTì— ìµœì¢… ì €ì¥ ì™„ë£Œ)ì— ëŒ€í•œ
    ìµœì¢… 100% ì§„í–‰ ë³´ê³ ë¥¼ Spring ê·œê²©ì— ë§ì¶° ì „ì†¡.

    â†’ ë‚´ë¶€ì ìœ¼ë¡œ report_progress_to_spring(type="UPLOAD_COMPLETE", progress=100.0, sizeBytes, checksum, durationSec) í˜¸ì¶œ.
    """
    try:
        file_size_bytes = final_file_path.stat().st_size
        member_id = member_id_override or settings.MEMBER_ID
        duration_sec = 0.0  # TODO: ffprobe ê²°ê³¼ë¡œ ì‹¤ì œ ì˜ìƒ ê¸¸ì´ ë„£ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì„œ êµì²´
    except Exception as e:
        logger.error(f"Failed to get file stats for final notification: {e}")
        file_size_bytes = 0
        member_id = member_id_override or settings.MEMBER_ID
        duration_sec = 0.0

    # UploadStatusì— UPLOAD_COMPLETEê°€ ì •ì˜ë¼ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì“°ê³ ,
    # í˜¹ì‹œ ì—†ìœ¼ë©´ ë¬¸ìì—´ ë¦¬í„°ëŸ´ë¡œ í´ë°±
    try:
        progress_type = UploadStatus.UPLOAD_COMPLETE.value  # type: ignore[attr-defined]
    except Exception:
        progress_type = "UPLOAD_COMPLETE"

    try:
        await report_progress_to_spring(
            job_id,
            progress_type,
            100.0,
            member_id=member_id,
            size_bytes=file_size_bytes,
            checksum=checksum,
            duration_sec=duration_sec,
        )
        logger.info(f"Final completion reported to Redis for Job {job_id}.")
    except Exception as e:
        logger.error(f"Failed to report final completion to Redis for Job {job_id}: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”íƒ€ ì €ì¥ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _save_job_meta(job_id: str, meta: Dict[str, Any]) -> None:
    """AI ì›Œì»¤/ë°±ì—”ë“œê°€ ì¡°íšŒí•  ìˆ˜ ìˆë„ë¡ ì‘ì—… ë©”íƒ€ë¥¼ Redisì— ì €ì¥."""
    try:
        redis: Redis = get_redis_client()
    except Exception as e:
        logger.warning(f"Redis unavailable; meta not saved for {job_id}: {e}")
        return
    try:
        await redis.set(get_meta_key(job_id), json.dumps(meta), ex=86400)  # 1 day
        logger.info(f"Saved job meta for {job_id}: {meta}")
    except Exception as e:
        logger.warning(f"Failed to save job meta for {job_id}: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI Worker ì—°ë™ ë¡œì§ (Redis Queue)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _trigger_ai_worker(
    job_id: str,
    final_file_path: Path,
    member_id: Optional[str],
    highlight_identifier: str,
) -> None:
    """
    AI Workerê°€ ì²˜ë¦¬í•  ì‘ì—… ìš”ì²­ì„ Redis List(Queue)ì— í‘¸ì‹œí•˜ê³ ,
    Spring ì„œë²„ì— 'í•˜ì´ë¼ì´íŠ¸ ìƒì„± ëŒ€ê¸°(QUEUEì— ìŒ“ì¸ ìƒíƒœ)' ì§„í–‰ ìƒíƒœë¥¼ ë³´ê³ .
    """
    try:
        redis: Redis = get_redis_client()
    except RedisConnectionError:
        logger.error(f"Redis not available, cannot queue AI task for Job {job_id}.")
        return
    except Exception as e:
        logger.error(f"Unknown error getting Redis client for Job {job_id}: {e}")
        return

    # 1. AI Workerì—ê²Œ ì „ë‹¬í•  í˜ì´ë¡œë“œ êµ¬ì„±
    ai_payload = AITaskPayload(
        jobId=job_id,
        memberId=(member_id or settings.MEMBER_ID),
        # originalFilePath=str(final_file_path.resolve())  # â¬…ï¸ ì´ì „(ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ˆëŒ€ ê²½ë¡œ ìœ„í—˜)
        originalFilePath=str(final_file_path),  # â¬…ï¸ ê³µìœ  ë³¼ë¥¨ ê²½ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬
    )

    # extra í•„ë“œ(ì›Œì»¤ ëª¨ë¸ì— ì—†ì–´ë„ ë¬´ì‹œë˜ë„ë¡ JSONì—ë§Œ í¬í•¨)
    payload_dict = json.loads(ai_payload.model_dump_json())
    payload_dict["highlightKey"] = highlight_identifier  # ì¶”ê°€ ë©”íƒ€

    try:
        queue_name = getattr(settings, "REDIS_QUEUE_NAME", "opencv-ai-job-queue")
        push_count = await redis.rpush(queue_name, json.dumps(payload_dict))
        logger.info(
            f"AI Task for Job {job_id} pushed to queue '{queue_name}'. "
            f"Queue size: {push_count}"
        )

        # ğŸ”¹ ìŠ¤í™ì— ë§ì¶°: í•˜ì´ë¼ì´íŠ¸ ìƒì„± ë‹¨ê³„ ì‹œì‘ ì „,
        #    type="PROCESSING", stage="QUEUED", progress=0 ìœ¼ë¡œ í•œ ë²ˆ ë³´ê³ 
        try:
            try:
                processing_type = UploadStatus.PROCESSING.value  # type: ignore[attr-defined]
            except Exception:
                processing_type = "PROCESSING"

            await report_progress_to_spring(
                job_id,
                processing_type,
                0.0,
                member_id=(member_id or settings.MEMBER_ID),
                stage="QUEUED",
                current_clip=0,
                total_clips=0,
            )
        except Exception as e:
            logger.error(f"Failed to report PROCESSING(QUEUED) for Job {job_id}: {e}")

    except Exception as e:
        logger.error(f"Failed to queue AI task for Job {job_id}: {e}")
        # ì—ëŸ¬ ì‹œì—ëŠ” ê¸°ì¡´ ERROR/FAILED í”Œë¡œìš°ì™€ í˜¸í™˜ë˜ë„ë¡ ë¬¸ìì—´ ì‚¬ìš©
        await report_progress_to_spring(job_id, UploadStatus.ERROR.value if hasattr(UploadStatus, "ERROR") else "FAILED", 0.0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§: ë³‘í•© ë° ì •ë¦¬ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def merge_chunks_and_cleanup(
    job_id: str,
    file_name: str,
    total_parts: int,
    chunk_dir: Path,
    member_id: Optional[str] = None,  # â† ê¸°ì¡´ í˜¸ì¶œê³¼ í˜¸í™˜
) -> None:
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ë³‘í•© ì‘ì—… ë¡œì§ì…ë‹ˆë‹¤.
    - ì²­í¬ë¥¼ ì„ì‹œ ê²½ë¡œì— ë³‘í•©í•œ ë’¤
    - ê³µìœ  ë³¼ë¥¨(SAVE_ROOT/{job_id}/{file_name})ìœ¼ë¡œ ì´ë™
    - ì™„ë£Œ ì •ë³´ Redis ë³´ê³  ë° AI Worker í íŠ¸ë¦¬ê±°

    ğŸ”¹ ì²­í¬ ì—…ë¡œë“œ êµ¬ê°„ì—ì„œ ì´ë¯¸ 0~90% ì§„í–‰ë¥ ì„ ë³´ê³ í•˜ê³  ìˆìœ¼ë¯€ë¡œ,
       ì´ í•¨ìˆ˜ì—ì„œëŠ”:
       - ë³‘í•© ì™„ë£Œ ì‹œ 99% (type=PROCESSING, stage="MERGE_COMPLETED")
       - ìµœì¢… ì €ì¥ ì™„ë£Œ ì‹œ 100% (type=UPLOAD_COMPLETE, sizeBytes/checksum/durationSec í¬í•¨)
       ë§Œ ì¶”ê°€ë¡œ ë³´ê³ .
    """
    # ìµœì¢… ì €ì¥ ë””ë ‰í† ë¦¬(SAVE_ROOT/{job_id})
    final_save_dir = Path(settings.SAVE_ROOT) / job_id
    final_save_dir.mkdir(parents=True, exist_ok=True)

    # ìµœì¢… ì €ì¥ ê²½ë¡œ(SAVE_ROOT/{job_id}/{file_name})
    final_save_path = final_save_dir / file_name

    # ì„ì‹œ ë³‘í•© ê²½ë¡œ(TEMP_ROOT/temp_{job_id}_{file_name})
    temp_merge_path = Path(settings.TEMP_ROOT) / f"temp_{job_id}_{file_name}"

    final_path: Optional[Path] = None
    calculated_checksum: Optional[str] = None

    # âš ï¸ ì—¬ê¸°ì„œëŠ” ë” ì´ìƒ 0% ì´ˆê¸°í™” í˜¸ì¶œì„ í•˜ì§€ ì•ŠìŒ
    #    (ì²­í¬ ì—…ë¡œë“œ ë‹¨ê³„ì—ì„œ ì´ë¯¸ 0~90%ë¥¼ ë³´ê³ í•˜ê³  ìˆìŒ)

    try:
        # 2) ì²­í¬ íŒŒì¼ ëª©ë¡ ì •ë ¬ ë° ê²€ì¦
        #   - ì—…ë¡œë“œ ë¼ìš°í„°ì—ì„œ '{job_id}_{file_name}.{part_index}' í˜•íƒœë¡œ ì €ì¥í–ˆë‹¤ê³  ê°€ì •
        chunk_files = sorted(chunk_dir.glob(f"{job_id}_{file_name}.*"))
        if len(chunk_files) != total_parts:
            raise Exception(
                f"Integrity check failure during merge. expected={total_parts}, actual={len(chunk_files)}"
            )

        logger.info(f"Starting merge of {total_parts} chunks into {temp_merge_path}")

        # 3) ì„ì‹œ ê²½ë¡œì— ë³‘í•©
        with temp_merge_path.open("wb") as outfile:
            for chunk_file in chunk_files:
                with chunk_file.open("rb") as infile:
                    shutil.copyfileobj(infile, outfile)

        logger.info("Merge completed at temp location. Moving to final save dir and calculating checksum.")

        # 3.2) ë³‘í•© ì™„ë£Œ ì‹œì : 99% (PROCESSING, stage=MERGE_COMPLETED)
        try:
            try:
                processing_type = UploadStatus.PROCESSING.value  # type: ignore[attr-defined]
            except Exception:
                processing_type = "PROCESSING"

            await report_progress_to_spring(
                job_id,
                processing_type,
                99.0,
                member_id=(member_id or settings.MEMBER_ID),
                stage="MERGE_COMPLETED",
                current_clip=0,
                total_clips=0,
            )
        except Exception as e:
            logger.error(f"Failed to report 99% PROCESSING for Job {job_id}: {e}")

        # 3.5) ìµœì¢… ê²½ë¡œë¡œ ì´ë™ (SAVE_ROOT/{jobId}/{file_name})
        shutil.move(str(temp_merge_path), str(final_save_path))
        final_path = final_save_path

        # 4) ì²´í¬ì„¬ ê³„ì‚°
        calculated_checksum = calculate_file_checksum(final_path)

        # 5) ìµœì¢… ì™„ë£Œ JSON ë³´ê³  (type=UPLOAD_COMPLETE, progress=100)
        await report_final_completion_to_spring(
            job_id,
            final_path,
            calculated_checksum,
            member_id_override=member_id,
        )

        # 6) AI Worker í í‘¸ì‹œ ë° ìƒíƒœ ë³´ê³  (í•˜ì´ë¼ì´íŠ¸í‚¤ í¬í•¨)
        await _trigger_ai_worker(job_id, final_path, member_id, highlight_identifier=str(uuid4()))

    except Exception as e:
        logger.error(
            f"Critical error during merge/cleanup/trigger for Job {job_id}: {e}",
            exc_info=True,
        )
        # ì—ëŸ¬ ì‹œ ê¸°ì¡´ ë¬¸ìì—´ ìƒíƒœ ìœ ì§€ ("FAILED") â€” ProgressTypeê³¼ëŠ” ë³„ê°œë¡œ ì—ëŸ¬ í‘œí˜„ìš©
        await report_progress_to_spring(job_id, "FAILED", 0.0)
    finally:
        # 7) ì„ì‹œ ì²­í¬ í´ë” ì‚­ì œ
        try:
            if chunk_dir.exists():
                shutil.rmtree(chunk_dir, ignore_errors=True)
                logger.info(f"Cleanup complete for Job {job_id}: removed chunk dir {chunk_dir}")
        except Exception as e:
            logger.warning(f"Failed to remove chunk dir {chunk_dir} for Job {job_id}: {e}")

        # 8) ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” ì„ì‹œ ë³‘í•© íŒŒì¼ ì œê±°(ì´ë™ ì‹¤íŒ¨ ë“±ì˜ ê²½ìš°)
        try:
            if temp_merge_path.exists():
                os.remove(temp_merge_path)
                logger.info(f"Cleanup complete for Job {job_id}: removed temp file {temp_merge_path}")
        except Exception as e:
            logger.warning(f"Failed to remove temp file {temp_merge_path} for Job {job_id}: {e}")
