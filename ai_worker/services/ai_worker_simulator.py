# ai_worker/services/ai_worker_simulator.py

import asyncio
import json
import logging
import os
import time
import subprocess
import cv2
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict
from redis.asyncio import Redis, ConnectionError as RedisConnectionError

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ëª¨ë“ˆ ë° ì„¤ì • ì„í¬íŠ¸/ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# íŒ¨í‚¤ì§€ ê¸°ì¤€ ì ˆëŒ€ ì„í¬íŠ¸ (ai_worker. ì ‘ë‘ì‚¬ ê³ ì •)
try:
    from ai_worker.utils.bh_edit import get_video_metadata
    from ai_worker.configs.registry import PLANS, DURATION_TOLERANCE_SEC, SIZE_TOLERANCE_BYTES
    from ai_worker.ai_modules.bh_geometry import compute_homography_auto, NBA
    from ai_worker.ai_modules.bh_detect import detect_ball_hsv
    # ì™¸ë¶€ ëª¨ë“ˆ: ìŠ¤í”„ë§ ì„œë²„ë¡œ ê²°ê³¼ ì „ì†¡ (ì¬ì‹œë„ í¬í•¨)
    from ai_worker.services.spring_reporter import send_highlight_result_with_retry, MAX_RETRIES
except ImportError as e:
    raise ImportError(
        "Failed to import ai_worker submodules. "
        "Ensure Dockerfile COPY paths place sources under /app/ai_worker and PYTHONPATH includes /app. "
        f"Original error: {e}"
    ) from e


class Settings(BaseModel):
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://:rlaehdus00123@host.docker.internal:6379/0")
    AI_QUEUE_NAME: str = os.getenv("REDIS_AI_JOB_QUEUE", "opencv-ai-job-queue")
    SPRING_API_URL: str = os.getenv("SPRING_API_URL", "http://host.docker.internal:8080/api/v1/jobs")
    OUTPUT_DIR: str = os.getenv("OUTPUT_DIR", "/data/highlights/processed")
    # FastAPI ìª½ REDIS_UPLOAD_PROGRESS_CHANNEL ê³¼ ê°™ì€ prefix ë¥¼ ì‚¬ìš©
    PROGRESS_CHANNEL_PREFIX: str = os.getenv("PROGRESS_CHANNEL_PREFIX", "opencv-progress-highlight")


settings = Settings()
os.makedirs(settings.OUTPUT_DIR, exist_ok=True)

_redis_client: Optional[Redis] = None


def get_redis_client() -> Redis:
    global _redis_client
    if _redis_client is None:
        try:
            _redis_client = Redis.from_url(settings.REDIS_URL, decode_responses=True)
        except Exception:
            raise RedisConnectionError("Redis connection failed.")
    return _redis_client


class AITaskPayload(BaseModel):
    model_config = ConfigDict(populate_by_name=True, extra="ignore")

    jobId: str = Field(..., description="ì‘ì—… ê³ ìœ  ID")
    memberId: str = Field(..., description="ìš”ì²­ ì‚¬ìš©ì ID")
    originalFilePath: str = Field(..., description="ì›ë³¸ íŒŒì¼ì˜ ì„œë²„ ë‚´ë¶€ ê²½ë¡œ")
    # FastAPI â†’ Redis: highlightKey  â†’ (alias ë§¤í•‘) â†’ highlightIdentifier(í•„ìˆ˜)
    highlightIdentifier: str = Field(..., alias="highlightKey", description="í•˜ì´ë¼ì´íŠ¸ ì‹ë³„ í‚¤(í•„ìˆ˜)")
    maxClips: Optional[int] = Field(default=None)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê³µí†µ: ì§„í–‰ë¥ /ì™„ë£Œ ë³´ê³  ìœ í‹¸ (Spring ProgressData ìŠ¤í™ ë§ì¶”ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_status_key(job_id: str) -> str:
    """FastAPI ìª½ê³¼ ë§ì¶”ê¸° ìœ„í•´ ë™ì¼í•œ ìŠ¤ëƒ…ìƒ· í‚¤ ì‚¬ìš©."""
    return f"job:{job_id}:status"


async def _publish_progress(job_id: str, payload: Dict[str, Any]) -> None:
    """
    - Redis SET (ìŠ¤ëƒ…ìƒ·)
    - Redis PUBLISH (Pub/Sub)
    ë‘˜ ë‹¤ ìˆ˜í–‰. FastAPI ìª½ê³¼ í˜•íƒœë¥¼ ë§ì¶˜ë‹¤.
    """
    redis = get_redis_client()
    status_key = _get_status_key(job_id)
    try:
        # ìŠ¤ëƒ…ìƒ·
        await redis.set(status_key, json.dumps(payload), ex=3600)
    except Exception as e:
        logger.error(f"[{job_id}] Failed to SET progress snapshot: {e}")

    try:
        # Pub/Sub
        channel_name = f"{settings.PROGRESS_CHANNEL_PREFIX}:{job_id}"
        await redis.publish(channel_name, json.dumps(payload))
        logger.info(
            f"[{job_id}] Progress PUBLISHED to {channel_name}: "
            f"type={payload.get('type')}, success={payload.get('success')}, "
            f"progress={payload.get('progress')}, message={payload.get('message')}"
        )
    except Exception as e:
        logger.error(f"[{job_id}] Failed to PUBLISH progress via Pub/Sub: {e}")


async def _report_processing_stage(
    job_id: str,
    member_id: str,
    stage: str,
    progress: float,
    current_clip: int,
    total_clips: int,
    highlight_id: Optional[str] = None,
) -> None:
    """
    ProgressType.PROCESSING ë‹¨ê³„ ë³´ê³ .

    ğŸ‘‰ ìµœì¢… Redis/PubSub JSON í˜•ì‹ (AI ì²˜ë¦¬ ì¤‘):

    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990002000,
      "type": "PROCESSING",
      "progress": 32.5,
      "jobId": "job1",
      "memberId": "xxxx"
    }

    stage / current_clip / total_clips / highlight_id ëŠ”
    - ë¡œê·¸ì—ë§Œ ì‚¬ìš©í•˜ê³ 
    - Redis payload ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    payload: Dict[str, Any] = {
        "status": 200,
        "success": True,
        "timeStamp": int(time.time() * 1000),
        "type": "PROCESSING",
        "memberId": str(member_id),
        "jobId": str(job_id),
        "progress": float(round(progress, 2)),
    }

    # ë””ë²„ê¹…ìš© ìƒì„¸ ë¡œê·¸ (Redis ë¡œëŠ” ì•ˆ ë‚˜ê°)
    logger.info(
        f"[{job_id}] PROCESSING stage={stage}, "
        f"clip={current_clip}/{total_clips}, "
        f"highlightIdentifier={highlight_id}, "
        f"progress={payload['progress']}"
    )

    await _publish_progress(job_id, payload)


async def _report_completion(
    job_id: str,
    member_id: str,
    success: bool,
    total_clips: int,
    highlight_id: Optional[str] = None,
    output_paths: Optional[List[str]] = None,
    error_message: Optional[str] = None,
) -> None:
    """
    ProgressType.COMPLETE ë‹¨ê³„ ë³´ê³ .

    ğŸ‘‰ ìµœì¢… Redis/PubSub JSON í˜•ì‹ (AI ì²˜ë¦¬ ì™„ë£Œ):

    (ì„±ê³µ)
    {
      "status": 200,
      "success": true,
      "timeStamp": 1731990003000,
      "type": "COMPLETE",
      "jobId": "job1",
      "memberId": "xxxx"
    }

    (ì—ëŸ¬)
    {
      "status": 200,
      "success": false,
      "timeStamp": 1731990003000,
      "type": "COMPLETE",
      "jobId": "job1",
      "memberId": "xxxx",
      "message": "ì—ëŸ¬ ì„¤ëª…"
    }

    - COMPLETE ë‹¨ê³„ì—ì„œëŠ” progress / stage / currentClip / totalClips ë“±ì€
      ì „ì†¡í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    progress_value = 100.0 if success else -1.0  # ë¡œê·¸ìš©

    payload: Dict[str, Any] = {
        "status": 200,
        "success": bool(success),
        "timeStamp": int(time.time() * 1000),
        "type": "COMPLETE",
        "memberId": str(member_id),
        "jobId": str(job_id),
    }

    # ì—ëŸ¬ ì‹œì—ëŠ” message í¬í•¨
    if not success and error_message:
        payload["message"] = str(error_message)

    await _publish_progress(job_id, payload)

    # ë¡œê·¸ëŠ” ê¸°ì¡´ ëŠë‚Œ ìœ ì§€
    status_str = "COMPLETED" if success else "FAILED"
    logger.info(
        f"[{job_id}] Final Status: {status_str} "
        f"(success={success}, totalClips={total_clips}, progress={progress_value})"
        f"{' | highlightIdentifier='+highlight_id if highlight_id else ''}"
    )
    if output_paths:
        logger.info(f"[{job_id}] Output files: {output_paths}")
    logger.info(f"[{job_id}] (Simulated) Final report target: {settings.SPRING_API_URL}/complete")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. í•µì‹¬ ë¡œì§: AI ì‹œë®¬ë ˆì´ì…˜, FFmpeg (ê¸°ëŠ¥ì€ ê·¸ëŒ€ë¡œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _find_matching_plan(metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    input_size = metadata.get("size_bytes", 0)
    input_duration = metadata.get("duration_sec", 0.0)
    input_width = metadata.get("width", 0)
    input_height = metadata.get("height", 0)

    for plan in PLANS:
        size_diff = abs(input_size - plan.get("size_bytes", 0))
        is_size_match = size_diff <= SIZE_TOLERANCE_BYTES

        duration_diff = abs(input_duration - plan.get("duration_sec", 0.0))
        is_duration_match = duration_diff <= DURATION_TOLERANCE_SEC

        is_resolution_match = (
            input_width == plan.get("width", 0)
            and input_height == plan.get("height", 0)
        )

        if is_size_match and is_duration_match and is_resolution_match:
            segments = [
                {"start": s, "duration": d, "label": l}
                for s, d, l in plan["segments"]
            ]
            return {"name": f"Demo Plan {plan['id']}", "segments": segments}
    return None


async def _run_ai_pipeline_simulation(
    job_id: str,
    original_path: str,
    metadata: Dict[str, Any],
    member_id: str,
    highlight_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    ê¸°ì¡´ AI íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ë‘ê³ ,
    ë¶„ì„ êµ¬ê°„(15% â†’ 70%) ë™ì•ˆ progress ë¥¼ ì´˜ì´˜í•˜ê²Œ ë³´ë‚´ë„ë¡ë§Œ ìˆ˜ì •.
    """
    start_time = time.time()
    try:
        logger.info(f"[{job_id}] [SIMU 1/5] Starting video decoding and AI pipeline initialization...")
        cap = cv2.VideoCapture(original_path)
        if not cap.isOpened():
            logger.error(
                f"[{job_id}] ERROR: Could not open video file for AI analysis simulation. "
                f"Path: {original_path}"
            )
            return []
        ret, frame = cap.read()
        cap.release()
        if not ret or frame is None:
            logger.error(f"[{job_id}] ERROR: Failed to read first frame from {original_path}")
            return []
        await asyncio.sleep(0.5)

        compute_homography_auto(frame=frame, spec=NBA)
        logger.info(f"[{job_id}] [SIMU 2/5] Court Geometry Analysis finished (simulated).")
        await asyncio.sleep(0.3)

        detect_ball_hsv(frame=frame)
        logger.info(f"[{job_id}] [SIMU 3/5] Object Detector run finished (simulated).")
        await asyncio.sleep(0.2)

        matching_plan = _find_matching_plan(metadata)
        if not matching_plan:
            analysis_time = (time.time() - start_time) + (metadata.get("duration_sec", 10.0) * 0.2)
            logger.warning(
                f"[{job_id}] [SIMU 4/5] AI Analysis Completed in {analysis_time:.2f}s, "
                f"but no highlights found. Metadata: {metadata}"
            )
            return []

        logger.info(f"[{job_id}] [SIMU 4/5] AI Analysis Success! Found matching plan: {matching_plan['name']}")
        logger.info(f"[{job_id}] Starting Frame-by-Frame Inference (Time Consuming Process)...")

        # --- ì—¬ê¸°ì„œë¶€í„°ê°€ ì‹¤ì œ 'ê¸´ ë¶„ì„ êµ¬ê°„' ì‹œë®¬ë ˆì´ì…˜ ---
        analysis_time_sim = metadata.get("duration_sec", 10.0) * 0.7

        # highlight_id ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„¸ë¶„í™”ëœ ì§„í–‰ë¥  ì „ì†¡
        if highlight_id:
            steps = 50  # Bì•ˆ: ì ë‹¹í•œ ë‹¨ê³„ ìˆ˜
            for step in range(steps):
                await asyncio.sleep(analysis_time_sim / steps)
                smooth_progress = 15 + ((step + 1) / steps * (70 - 15))  # 15% â†’ 70%
                await _report_processing_stage(
                    job_id=job_id,
                    member_id=member_id,
                    stage="ANALYZING",
                    progress=smooth_progress,
                    current_clip=0,
                    total_clips=0,
                    highlight_id=highlight_id,
                )
        else:
            # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ highlight_id ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ í•œ ë²ˆì— sleep
            await asyncio.sleep(analysis_time_sim)

        logger.info(
            f"[{job_id}] [SIMU 5/5] Inference finished in {analysis_time_sim:.2f}s. "
            f"{len(matching_plan['segments'])} segments identified."
        )
        return matching_plan["segments"]

    except Exception as e:
        logger.critical(f"[{job_id}] AI PIPELINE CRASHED: {type(e).__name__}: {e}")
        return []


def _run_ffmpeg_cut(job_id: str, src_path: str, segment: Dict[str, Any], output_path: str) -> None:
    """
    í•˜ì´ë¼ì´íŠ¸ í´ë¦½ì„ ìƒì„±í•  ë•Œ,
    - ì›ë³¸ ì½”ë±/í•´ìƒë„ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ë˜ ê¸°ì¡´ ë°©ì‹(-c copy)ì„ ë²„ë¦¬ê³ 
    - ëª¨ë°”ì¼/ì›¹ í”Œë ˆì´ì–´ í˜¸í™˜ì„±ì´ ë†’ì€ í‘œì¤€ H.264 + AAC + yuv420p ë¡œ ì¬ì¸ì½”ë”©.
      (í•´ìƒë„ëŠ” ê°€ë¡œ 1280 ê³ ì •, ì„¸ë¡œëŠ” ë¹„ìœ¨ ìœ ì§€ & ì§ìˆ˜)
    """
    start = max(0.0, float(segment["start"]))
    duration = max(0.1, float(segment["duration"]))

    cmd = [
        "ffmpeg",
        "-y",
        "-ss",
        f"{start:.3f}",
        "-i",
        src_path,
        "-t",
        f"{duration:.3f}",
        # â–¼ ì¬ì¸ì½”ë”© ì„¤ì • (ëª¨ë°”ì¼/Expo-AV/ExoPlayer ì¹œí™”)
        # í•´ìƒë„: ê°€ë¡œ 1280, ì„¸ë¡œëŠ” ë¹„ìœ¨ ìœ ì§€ + ì§ìˆ˜(-2)
        "-vf",
        "scale=1280:720:force_original_aspect_ratio=decrease,pad=1280:720:(ow-iw)/2:(oh-ih)/2,setsar=1",
        # ë¹„ë””ì˜¤ ì½”ë±: H.264
        "-c:v",
        "libx264",
        "-preset",
        "veryfast",
        "-profile:v",
        "high",
        "-pix_fmt",
        "yuv420p",
        # ì˜¤ë””ì˜¤ ì½”ë±: AAC
        "-c:a",
        "aac",
        "-b:a",
        "128k",
        "-ac",
        "2",
        # MP4 ìŠ¤íŠ¸ë¦¬ë°/ëª¨ë°”ì¼ ì¬ìƒ ì¹œí™” ì˜µì…˜
        "-movflags",
        "+faststart",
        output_path,
    ]
    try:
        result = subprocess.run(
            cmd,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        logger.info(f"[{job_id}] Segment cut & re-encoded successfully: {output_path} ({segment['label']})")
    except subprocess.CalledProcessError as e:
        logger.error(
            f"[{job_id}] FFmpeg cutting/re-encoding failed.\n"
            f"Command: {' '.join(cmd)}\n"
            f"STDERR: {e.stderr.decode(errors='ignore')}"
        )
        raise RuntimeError(f"FFmpeg cut failed for segment: {segment['label']}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Task ì²˜ë¦¬ ë¡œì§ (ê¸°ëŠ¥ ìœ ì§€, ì§„í–‰ë¥ ë§Œ ìƒˆ ìŠ¤í™ìœ¼ë¡œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def process_task(task: AITaskPayload):
    job_id = task.jobId
    member_id = task.memberId
    original_path = task.originalFilePath
    highlight_id = (task.highlightIdentifier or "").strip()

    if not highlight_id:
        # í•˜ì´ë¼ì´íŠ¸ í‚¤ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì‹¤íŒ¨ (COMPLETE + success:false + message)
        await _report_completion(
            job_id=job_id,
            member_id=member_id,
            success=False,
            total_clips=0,
            highlight_id=None,
            output_paths=[],
            error_message="highlightKey/highlightIdentifier is missing in payload.",
        )
        logger.error(f"[{job_id}] Task FAILED: highlightKey/highlightIdentifier is missing in payload.")
        return

    # íŒŒì¼ ê²½ë¡œ + ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ í•¨ê»˜ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸
    output_files_with_segments: List[Dict[str, Any]] = []

    try:
        logger.info(
            f"[{job_id}] Task Received and Starting for member {member_id}. "
            f"File: {original_path} | highlightIdentifier={highlight_id}"
        )

        # 1ë‹¨ê³„: ì‘ì—… ìˆ˜ì‹  (1%)
        await _report_processing_stage(
            job_id=job_id,
            member_id=member_id,
            stage="JOB_RECEIVED_INIT",
            progress=1.0,
            current_clip=0,
            total_clips=0,
            highlight_id=highlight_id,
        )

        logger.info(f"[{job_id}] Simulating initial video file load and decoding setup (1.0s delay)...")
        await asyncio.sleep(1.0)

        # 2ë‹¨ê³„: ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ (5%)
        await _report_processing_stage(
            job_id=job_id,
            member_id=member_id,
            stage="VIDEO_LOAD_INIT",
            progress=5.0,
            current_clip=0,
            total_clips=0,
            highlight_id=highlight_id,
        )

        if not os.path.exists(original_path):
            try:
                base_dir = os.path.dirname(original_path)
                logger.error(f"[{job_id}] Task FAILED: Original file not found at {original_path}")
                logger.error(f"[{job_id}] DEBUG: base_dir exists? {os.path.exists(base_dir)}")
                logger.error(f"[{job_id}] DEBUG: /data/highlights exists? {os.path.exists('/data/highlights')}")
                if os.path.exists('/data/highlights'):
                    logger.error(f"[{job_id}] DEBUG: /data/highlights entries: {os.listdir('/data/highlights')}")
            except Exception as debug_e:
                logger.error(f"[{job_id}] DEBUG: error while listing /data/highlights: {debug_e}")

            raise FileNotFoundError(f"Original file not found: {original_path}")

        # 3ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (15%)
        metadata = get_video_metadata(original_path)
        logger.info(
            f"[{job_id}] Metadata: Duration={metadata.get('duration_sec'):.2f}s, "
            f"Size={metadata.get('size_bytes')}, "
            f"Resolution={metadata.get('width')}x{metadata.get('height')}"
        )
        await _report_processing_stage(
            job_id=job_id,
            member_id=member_id,
            stage="METADATA_EXTRACTED",
            progress=15.0,
            current_clip=0,
            total_clips=0,
            highlight_id=highlight_id,
        )

        # 4ë‹¨ê³„: ë¶„ì„ êµ¬ê°„ (15 â†’ 70% êµ¬ê°„ì€ ë‚´ë¶€ì—ì„œ ì´˜ì´˜í•˜ê²Œ ë³´ê³ )
        highlight_segments = await _run_ai_pipeline_simulation(
            job_id=job_id,
            original_path=original_path,
            metadata=metadata,
            member_id=member_id,
            highlight_id=highlight_id,
        )

        # ë¶„ì„ ìµœì¢… ë§ˆë¬´ë¦¬ í¬ì¸íŠ¸ 70% (ê¸°ì¡´ í˜¸í™˜ìš©)
        await _report_processing_stage(
            job_id=job_id,
            member_id=member_id,
            stage="ANALYSIS_FINISHED",
            progress=70.0,
            current_clip=0,
            total_clips=0,
            highlight_id=highlight_id,
        )

        # 5ë‹¨ê³„: í•˜ì´ë¼ì´íŠ¸ ì»·íŒ… (70 â†’ 99%)
        if not highlight_segments:
            logger.warning(f"[{job_id}] AI analysis completed successfully, but found no highlights to cut.")
            total_segments = 0
        else:
            logger.info(f"[{job_id}] Starting video cutting for {len(highlight_segments)} segments.")

            # âœ… jobIdë³„ í•˜ìœ„ í´ë” ìƒì„±: /data/highlights/processed/{jobId}
            job_output_dir = os.path.join(settings.OUTPUT_DIR, job_id)
            os.makedirs(job_output_dir, exist_ok=True)

            total_segments = len(highlight_segments)
            for i, segment in enumerate(highlight_segments):
                # íŒŒì¼ ì´ë¦„ì€ ê¸°ì¡´ íŒ¨í„´ ìœ ì§€ (jobId_segment_xx.mp4)
                output_filename = f"{job_id}_segment_{i+1:02d}.mp4"
                output_path = os.path.join(job_output_dir, output_filename)

                _run_ffmpeg_cut(job_id, original_path, segment, output_path)

                # íŒŒì¼ ê²½ë¡œì™€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ ë¬¶ì–´ ì €ì¥ (spring_reporter ë¡œ ì „ë‹¬)
                output_files_with_segments.append(
                    {
                        "output_path": output_path,
                        "segment": segment,
                    }
                )

                # ğŸ”¹ ì»·íŒ… êµ¬ê°„ progress: 70% â†’ 99% ì‚¬ì´ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë°°
                cut_progress = 70 + ((i + 1) / total_segments * (99 - 70))
                await _report_processing_stage(
                    job_id=job_id,
                    member_id=member_id,
                    stage="CUTTING",
                    progress=cut_progress,
                    current_clip=i + 1,
                    total_clips=total_segments,
                    highlight_id=highlight_id,
                )

        # 6ë‹¨ê³„: ì™„ë£Œ ë³´ê³  (HTTP ì „ì†¡ + COMPLETE ì´ë²¤íŠ¸)
        final_output_paths = [d["output_path"] for d in output_files_with_segments]

        if not output_files_with_segments:
            # í•˜ì´ë¼ì´íŠ¸ê°€ ì—†ìœ¼ë©´ HTTP ì „ì†¡ ì—†ì´ COMPLETE ë§Œ ë³´ê³  (ì„±ê³µ)
            await _report_completion(
                job_id=job_id,
                member_id=member_id,
                success=True,
                total_clips=0,
                highlight_id=highlight_id,
                output_paths=[],
            )
            final_message = "Task processing COMPLETE. No highlights found."
        else:
            # í•˜ì´ë¼ì´íŠ¸ê°€ ìˆìœ¼ë©´ HTTP ì „ì†¡ ì‹œë„ (ì¬ì‹œë„ í¬í•¨)
            http_success = await send_highlight_result_with_retry(
                job_id,                # âœ… jobId ë°”ë””ì— ë„£ê¸° ìœ„í•´ ì¶”ê°€
                member_id,
                highlight_id,
                output_files_with_segments,
            )

            # HTTP ì „ì†¡ ê²°ê³¼ì— ê´€ê³„ì—†ì´ COMPLETE ì´ë²¤íŠ¸ëŠ” ì„±ê³µìœ¼ë¡œ ê°„ì£¼
            await _report_completion(
                job_id=job_id,         # âœ… íŒŒë¼ë¯¸í„° ì´ë¦„ ì •í™•íˆ job_id
                member_id=member_id,
                success=True,
                total_clips=len(output_files_with_segments),
                highlight_id=highlight_id,
                output_paths=final_output_paths,
            )

            if http_success:
                final_message = "Task processing COMPLETE and SUCCESSFULLY REPORTED to Spring."
            else:
                final_message = (
                    f"Task processing COMPLETE but HTTP REPORTING FAILED after {MAX_RETRIES} retries."
                )

        logger.info(f"[{job_id}] {final_message}. Total output files: {len(output_files_with_segments)}")

    except FileNotFoundError as e:
        await _report_completion(
            job_id=job_id,
            member_id=member_id,
            success=False,
            total_clips=0,
            highlight_id=highlight_id,
            output_paths=[],
            error_message=str(e),
        )
        logger.error(f"[{job_id}] Task FAILED: Original file not found at {original_path}")
    except Exception as e:
        await _report_completion(
            job_id=job_id,
            member_id=member_id,
            success=False,
            total_clips=0,
            highlight_id=highlight_id,
            output_paths=[],
            error_message=f"Unexpected error in AI worker: {type(e).__name__}: {e}",
        )
        logger.error(f"[{job_id}] Task FAILED unexpectedly: {type(e).__name__}: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Main Worker ë£¨í”„ (ìˆ˜ì‹ ì/Consumer ë¡œì§)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def ai_worker_main():
    logger.info("--- AI Worker Simulator (Consumer) Started ---")
    logger.info(f"Monitoring queue: {settings.AI_QUEUE_NAME}")

    redis = get_redis_client()

    while True:
        try:
            result = await redis.blpop(settings.AI_QUEUE_NAME, timeout=0)
            if result:
                queue_name, json_data = result
                try:
                    task = AITaskPayload.model_validate_json(json_data)
                    logger.info(f"New job received from queue: {task.jobId}")
                    asyncio.create_task(process_task(task))
                except json.JSONDecodeError:
                    logger.error(f"Invalid JSON payload received: {json_data}")
                except Exception as e:
                    logger.error(f"Error processing task: {e}")
        except RedisConnectionError:
            logger.error("Lost connection to Redis. Retrying in 5 seconds...")
            await asyncio.sleep(5)
        except KeyboardInterrupt:
            logger.info("Worker stopped by user.")
            break
        except Exception as e:
            logger.error(f"An unexpected error occurred in worker loop: {e}")
            await asyncio.sleep(1)


if __name__ == "__main__":
    try:
        asyncio.run(ai_worker_main())
    except RedisConnectionError:
        logger.critical("AI Worker cannot start without Redis connection. Please check Redis URL.")
    except Exception as e:
        logger.error(f"Worker crashed during startup: {e}")
