# ai_worker/services/ai_worker_simulator.py

import asyncio
import json
import logging
import os
import time
import subprocess
import cv2
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, ConfigDict
from redis.asyncio import Redis, ConnectionError as RedisConnectionError

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ëª¨ë“ˆ ë° ì„¤ì • ì„í¬íŠ¸/ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# íŒ¨í‚¤ì§€ ê¸°ì¤€ ì ˆëŒ€ ì„í¬íŠ¸ (ai_worker. ì ‘ë‘ì‚¬ ê³ ì •)
try:
    from ai_worker.utils.bh_edit import get_video_metadata
    from ai_worker.configs.registry import PLANS, DURATION_TOLERANCE_SEC, SIZE_TOLERANCE_BYTES
    from ai_worker.ai_modules.bh_geometry import compute_homography_auto, NBA
    from ai_worker.ai_modules.bh_detect import detect_ball_hsv
    # ì™¸ë¶€ ëª¨ë“ˆ: ìŠ¤í”„ë§ ì„œë²„ë¡œ ê²°ê³¼ ì „ì†¡ (ì¬ì‹œë„ í¬í•¨)
    from ai_worker.services.spring_reporter import send_highlight_result_with_retry, MAX_RETRIES
except ImportError as e:
    raise ImportError(
        "Failed to import ai_worker submodules. "
        "Ensure Dockerfile COPY paths place sources under /app/ai_worker and PYTHONPATH includes /app. "
        f"Original error: {e}"
    ) from e


class Settings(BaseModel):
    REDIS_URL: str = os.getenv("REDIS_URL", "redis://host.docker.internal:6379/0")
    AI_QUEUE_NAME: str = os.getenv("REDIS_AI_JOB_QUEUE", "opencv-ai-job-queue")
    SPRING_API_URL: str = os.getenv("SPRING_API_URL", "http://host.docker.internal:8080/api/v1/jobs")
    OUTPUT_DIR: str = os.getenv("OUTPUT_DIR", "/data/highlights/processed")
    PROGRESS_CHANNEL_PREFIX: str = os.getenv("PROGRESS_CHANNEL_PREFIX", "opencv-progress-highlight")


settings = Settings()
os.makedirs(settings.OUTPUT_DIR, exist_ok=True)

_redis_client: Optional[Redis] = None


def get_redis_client() -> Redis:
    global _redis_client
    if _redis_client is None:
        try:
            _redis_client = Redis.from_url(settings.REDIS_URL, decode_responses=True)
        except Exception:
            raise RedisConnectionError("Redis connection failed.")
    return _redis_client


class AITaskPayload(BaseModel):
    model_config = ConfigDict(populate_by_name=True, extra="ignore")

    jobId: str = Field(..., description="ì‘ì—… ê³ ìœ  ID")
    memberId: str = Field(..., description="ìš”ì²­ ì‚¬ìš©ì ID")
    originalFilePath: str = Field(..., description="ì›ë³¸ íŒŒì¼ì˜ ì„œë²„ ë‚´ë¶€ ê²½ë¡œ")
    # FastAPI â†’ Redis: highlightKey  â†’ (alias ë§¤í•‘) â†’ highlightIdentifier(í•„ìˆ˜)
    highlightIdentifier: str = Field(..., alias="highlightKey", description="í•˜ì´ë¼ì´íŠ¸ ì‹ë³„ í‚¤(í•„ìˆ˜)")
    maxClips: Optional[int] = Field(default=None)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. í•µì‹¬ ë¡œì§: AI ì‹œë®¬ë ˆì´ì…˜, FFmpeg, ë³´ê³  í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _find_matching_plan(metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    input_size = metadata.get("size_bytes", 0)
    input_duration = metadata.get("duration_sec", 0.0)
    input_width = metadata.get("width", 0)
    input_height = metadata.get("height", 0)

    for plan in PLANS:
        size_diff = abs(input_size - plan.get("size_bytes", 0))
        is_size_match = size_diff <= SIZE_TOLERANCE_BYTES

        duration_diff = abs(input_duration - plan.get("duration_sec", 0.0))
        is_duration_match = duration_diff <= DURATION_TOLERANCE_SEC

        is_resolution_match = (input_width == plan.get("width", 0) and input_height == plan.get("height", 0))

        if is_size_match and is_duration_match and is_resolution_match:
            segments = [{"start": s, "duration": d, "label": l} for s, d, l in plan["segments"]]
            return {"name": f"Demo Plan {plan['id']}", "segments": segments}
    return None


async def _run_ai_pipeline_simulation(
    job_id: str,
    original_path: str,
    metadata: Dict[str, Any],
    highlight_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    ê¸°ì¡´ AI íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ë‘ê³ ,
    ë¶„ì„ êµ¬ê°„(15% â†’ 70%) ë™ì•ˆ progressë¥¼ ì´˜ì´˜í•˜ê²Œ ë³´ë‚´ë„ë¡ ìˆ˜ì •.
    """
    start_time = time.time()
    try:
        logger.info(f"[{job_id}] [SIMU 1/5] Starting video decoding and AI pipeline initialization...")
        cap = cv2.VideoCapture(original_path)
        if not cap.isOpened():
            logger.error(f"[{job_id}] ERROR: Could not open video file for AI analysis simulation. Path: {original_path}")
            return []
        ret, frame = cap.read()
        cap.release()
        if not ret or frame is None:
            logger.error(f"[{job_id}] ERROR: Failed to read first frame from {original_path}")
            return []
        await asyncio.sleep(0.5)

        compute_homography_auto(frame=frame, spec=NBA)
        logger.info(f"[{job_id}] [SIMU 2/5] Court Geometry Analysis finished (simulated).")
        await asyncio.sleep(0.3)

        detect_ball_hsv(frame=frame)
        logger.info(f"[{job_id}] [SIMU 3/5] Object Detector run finished (simulated).")
        await asyncio.sleep(0.2)

        matching_plan = _find_matching_plan(metadata)
        if not matching_plan:
            analysis_time = (time.time() - start_time) + (metadata.get("duration_sec", 10.0) * 0.2)
            logger.warning(
                f"[{job_id}] [SIMU 4/5] AI Analysis Completed in {analysis_time:.2f}s, "
                f"but no highlights found. Metadata: {metadata}"
            )
            return []

        logger.info(f"[{job_id}] [SIMU 4/5] AI Analysis Success! Found matching plan: {matching_plan['name']}")
        logger.info(f"[{job_id}] Starting Frame-by-Frame Inference (Time Consuming Process)...")

        # --- ì—¬ê¸°ì„œë¶€í„°ê°€ ì‹¤ì œ 'ê¸´ ë¶„ì„ êµ¬ê°„' ì‹œë®¬ë ˆì´ì…˜ ---
        analysis_time_sim = metadata.get("duration_sec", 10.0) * 0.7

        # highlight_idê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„¸ë¶„í™”ëœ ì§„í–‰ë¥  ì „ì†¡
        if highlight_id:
            steps = 50  # Bì•ˆ: ì ë‹¹í•œ ë‹¨ê³„ ìˆ˜
            for step in range(steps):
                await asyncio.sleep(analysis_time_sim / steps)
                smooth_progress = 15 + ((step + 1) / steps * (70 - 15))  # 15% â†’ 70%
                await _report_progress(job_id, "ANALYZING", smooth_progress, highlight_id=highlight_id)
        else:
            # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ highlight_idê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ í•œ ë²ˆì— sleep
            await asyncio.sleep(analysis_time_sim)

        logger.info(
            f"[{job_id}] [SIMU 5/5] Inference finished in {analysis_time_sim:.2f}s. "
            f"{len(matching_plan['segments'])} segments identified."
        )
        return matching_plan["segments"]

    except Exception as e:
        logger.critical(f"[{job_id}] AI PIPELINE CRASHED: {type(e).__name__}: {e}")
        return []


def _run_ffmpeg_cut(job_id: str, src_path: str, segment: Dict[str, Any], output_path: str) -> None:
    start = max(0.0, float(segment["start"]))
    duration = max(0.1, float(segment["duration"]))

    cmd = [
        "ffmpeg", "-y", "-ss", f"{start:.3f}", "-i", src_path,
        "-t", f"{duration:.3f}", "-c", "copy",
        "-avoid_negative_ts", "make_zero", output_path,
    ]
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        logger.info(f"[{job_id}] Segment cut success: {output_path} ({segment['label']})")
    except subprocess.CalledProcessError as e:
        logger.error(f"[{job_id}] FFmpeg cutting failed: {e.stderr.decode()}")
        raise RuntimeError(f"FFmpeg cut failed for segment: {segment['label']}")


async def _report_progress(job_id: str, status: str, progress: float, highlight_id: Optional[str] = None):
    redis = get_redis_client()
    try:
        channel_name = f"{settings.PROGRESS_CHANNEL_PREFIX}:{job_id}"
        payload = {"jobId": job_id, "status": status, "progress": round(progress, 2)}
        if highlight_id:
            payload["highlightIdentifier"] = highlight_id
        await redis.publish(channel_name, json.dumps(payload))
        logger.info(
            f"[{job_id}] Progress published to {channel_name}: {status}, {progress:.1f}%"
            f"{' | highlightIdentifier='+highlight_id if highlight_id else ''}"
        )
    except Exception as e:
        logger.error(f"[{job_id}] Failed to report progress via Pub/Sub: {e}")


async def _report_completion(job_id: str, success: bool, output_paths: List[str] = None, highlight_id: Optional[str] = None):
    status = "COMPLETED" if success else "FAILED"
    await _report_progress(job_id, status, 100.0 if success else -1.0, highlight_id=highlight_id)

    payload = {
        "jobId": job_id,
        "status": status,
        "resultFiles": output_paths or [],
        "message": "AI analysis and video cutting finished." if success else "AI processing failed."
    }
    if highlight_id:
        payload["highlightIdentifier"] = highlight_id

    logger.info(f"[{job_id}] Final Status: {status}"
                f"{' | highlightIdentifier='+highlight_id if highlight_id else ''}")
    logger.info(f"[{job_id}] (Simulated) Final report target: {settings.SPRING_API_URL}/complete")


async def process_task(task: AITaskPayload):
    job_id = task.jobId
    original_path = task.originalFilePath
    highlight_id = (task.highlightIdentifier or "").strip()

    if not highlight_id:
        # í•˜ì´ë¼ì´íŠ¸ í‚¤ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì‹¤íŒ¨ (ë³€ê²½ ì—†ìŒ)
        await _report_completion(job_id, success=False, highlight_id=None)
        logger.error(f"[{job_id}] Task FAILED: highlightKey/highlightIdentifier is missing in payload.")
        return

    # íŒŒì¼ ê²½ë¡œ + ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ í•¨ê»˜ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸
    output_files_with_segments: List[Dict[str, Any]] = []

    try:
        logger.info(
            f"[{job_id}] Task Received and Starting for member {task.memberId}. "
            f"File: {original_path} | highlightIdentifier={highlight_id}"
        )

        # 1% / 5% / 15% êµ¬ê°„ì€ ê¸°ì¡´ ê·¸ëŒ€ë¡œ ìœ ì§€
        await _report_progress(job_id, "JOB_RECEIVED_INIT", 1, highlight_id=highlight_id)
        logger.info(f"[{job_id}] Simulating initial video file load and decoding setup (1.0s delay)...")
        await asyncio.sleep(1.0)
        await _report_progress(job_id, "VIDEO_LOAD_INIT", 5, highlight_id=highlight_id)

        if not os.path.exists(original_path):
            try:
                base_dir = os.path.dirname(original_path)
                logger.error(f"[{job_id}] Task FAILED: Original file not found at {original_path}")
                logger.error(f"[{job_id}] DEBUG: base_dir exists? {os.path.exists(base_dir)}")
                logger.error(f"[{job_id}] DEBUG: /data/highlights exists? {os.path.exists('/data/highlights')}")
                if os.path.exists('/data/highlights'):
                     logger.error(f"[{job_id}] DEBUG: /data/highlights entries: {os.listdir('/data/highlights')}")
            except Exception as debug_e:
                logger.error(f"[{job_id}] DEBUG: error while listing /data/highlights: {debug_e}")

            raise FileNotFoundError(f"Original file not found: {original_path}")

        metadata = get_video_metadata(original_path)
        logger.info(
            f"[{job_id}] Metadata: Duration={metadata.get('duration_sec'):.2f}s, "
            f"Size={metadata.get('size_bytes')}, Resolution={metadata.get('width')}x{metadata.get('height')}"
        )
        await _report_progress(job_id, "METADATA_EXTRACTED", 15, highlight_id=highlight_id)

        # ğŸ”¹ ë¶„ì„ íŒŒíŠ¸(15â†’70)ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì„¸ë¶„í™”í•´ì„œ progress ì „ì†¡
        highlight_segments = await _run_ai_pipeline_simulation(
            job_id,
            original_path,
            metadata,
            highlight_id=highlight_id,
        )

        # ğŸ”¹ ë§ˆì§€ë§‰ì— ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ 70% ì§€ì  í•œ ë²ˆ ë” ì°ì–´ì„œ í˜¸í™˜ì„± ìœ ì§€
        await _report_progress(job_id, "ANALYSIS_FINISHED", 70, highlight_id=highlight_id)

        # 4ë‹¨ê³„: í•˜ì´ë¼ì´íŠ¸ ì»·íŒ…
        if not highlight_segments:
            logger.warning(f"[{job_id}] AI analysis completed successfully, but found no highlights to cut.")
        else:
            logger.info(f"[{job_id}] Starting video cutting for {len(highlight_segments)} segments.")

            # âœ… jobIdë³„ í•˜ìœ„ í´ë” ìƒì„±: /data/highlights/processed/{jobId}
            job_output_dir = os.path.join(settings.OUTPUT_DIR, job_id)
            os.makedirs(job_output_dir, exist_ok=True)

            total_segments = len(highlight_segments)
            for i, segment in enumerate(highlight_segments):
                # íŒŒì¼ ì´ë¦„ì€ ê¸°ì¡´ íŒ¨í„´ ìœ ì§€ (jobId_segment_xx.mp4) â†’ ë‹¤ë¥¸ ë¡œì§ ì˜í–¥ ìµœì†Œí™”
                output_filename = f"{job_id}_segment_{i+1:02d}.mp4"
                # âœ… ì´ì œëŠ” jobId í´ë” ì•ˆì— ì €ì¥
                output_path = os.path.join(job_output_dir, output_filename)

                _run_ffmpeg_cut(job_id, original_path, segment, output_path)

                # íŒŒì¼ ê²½ë¡œì™€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ë¥¼ ë¬¶ì–´ ì €ì¥ (spring_reporterë¡œ ì „ë‹¬)
                output_files_with_segments.append({
                    "output_path": output_path,
                    "segment": segment
                })

                # ğŸ”¹ ì»·íŒ… êµ¬ê°„ progress: 70% â†’ 99% ì‚¬ì´ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë°°
                cut_progress = 70 + ((i + 1) / total_segments * (99 - 70))
                await _report_progress(job_id, "CUTTING", cut_progress, highlight_id=highlight_id)

        # 5ë‹¨ê³„: ì™„ë£Œ ë³´ê³  (HTTP ì „ì†¡ + Pub/Sub)
        final_output_paths = [d["output_path"] for d in output_files_with_segments]

        if not output_files_with_segments:
            # í•˜ì´ë¼ì´íŠ¸ê°€ ì—†ìœ¼ë©´ HTTP ì „ì†¡ ì—†ì´ Pub/Subìœ¼ë¡œë§Œ ì™„ë£Œ ë³´ê³ 
            await _report_completion(job_id, success=True, highlight_id=highlight_id)
            final_message = "Task processing COMPLETE. No highlights found."
        else:
            # í•˜ì´ë¼ì´íŠ¸ê°€ ìˆìœ¼ë©´ HTTP ì „ì†¡ ì‹œë„ (ì¬ì‹œë„ í¬í•¨)
            http_success = await send_highlight_result_with_retry(
                task.memberId,
                highlight_id,
                output_files_with_segments
            )

            # HTTP ì „ì†¡ ê²°ê³¼ì— ê´€ê³„ì—†ì´ Pub/Subìœ¼ë¡œ ìµœì¢… ì™„ë£Œ ìƒíƒœ ë³´ê³  (AI ì‘ì—… ì„±ê³µìœ¼ë¡œ ê°„ì£¼)
            await _report_completion(
                job_id,
                success=True,
                output_paths=final_output_paths,
                highlight_id=highlight_id
            )

            if http_success:
                final_message = "Task processing COMPLETE and SUCCESSFULLY REPORTED to Spring."
            else:
                final_message = f"Task processing COMPLETE but HTTP REPORTING FAILED after {MAX_RETRIES} retries."

        logger.info(f"[{job_id}] {final_message}. Total output files: {len(output_files_with_segments)}")

    except FileNotFoundError:
        await _report_completion(job_id, success=False, highlight_id=highlight_id)
        logger.error(f"[{job_id}] Task FAILED: Original file not found at {original_path}")
    except Exception as e:
        await _report_completion(job_id, success=False, highlight_id=highlight_id)
        logger.error(f"[{job_id}] Task FAILED unexpectedly: {type(e).__name__}: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Main Worker ë£¨í”„ (ìˆ˜ì‹ ì/Consumer ë¡œì§)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def ai_worker_main():
    logger.info("--- AI Worker Simulator (Consumer) Started ---")
    logger.info(f"Monitoring queue: {settings.AI_QUEUE_NAME}")

    redis = get_redis_client()

    while True:
        try:
            result = await redis.blpop(settings.AI_QUEUE_NAME, timeout=0)
            if result:
                queue_name, json_data = result
                try:
                    task = AITaskPayload.model_validate_json(json_data)
                    logger.info(f"New job received from queue: {task.jobId}")
                    asyncio.create_task(process_task(task))
                except json.JSONDecodeError:
                    logger.error(f"Invalid JSON payload received: {json_data}")
                except Exception as e:
                    logger.error(f"Error processing task: {e}")
        except RedisConnectionError:
            logger.error("Lost connection to Redis. Retrying in 5 seconds...")
            await asyncio.sleep(5)
        except KeyboardInterrupt:
            logger.info("Worker stopped by user.")
            break
        except Exception as e:
            logger.error(f"An unexpected error occurred in worker loop: {e}")
            await asyncio.sleep(1)


if __name__ == "__main__":
    try:
        asyncio.run(ai_worker_main())
    except RedisConnectionError:
        logger.critical("AI Worker cannot start without Redis connection. Please check Redis URL.")
    except Exception as e:
        logger.error(f"Worker crashed during startup: {e}")
